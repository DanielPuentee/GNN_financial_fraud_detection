{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Daniel Puente Viejo*  \n",
    "\n",
    "<img src=\"https://cdn-icons-png.flaticon.com/512/5043/5043998.png\" width=\"100\" height=\"100\" float =\"right\">    \n",
    "\n",
    "This notebook explains the steps to generate the graph by applying the data already analysed and cleaned. The code on how to include new users is also provided.\n",
    "In any case, we have provided a series of scripts where you can run the latter with just one function: `graphs_management.py` \n",
    "- <a href='#1'><ins>1. Loading of Libraries and Data<ins></a>\n",
    "- <a href='#2'><ins>2. Split data<ins></a>\n",
    "- <a href='#2'><ins>3. Graph creation<ins></a>\n",
    "- <a href='#4'><ins>4. Graph save<ins> </a>\n",
    "- <a href='#5'><ins>5. New customer inclusion<ins> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'>1. Loading of Libraries and Data</a>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\tfg_mlflow\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import Sequential, Linear, SAGEConv, to_hetero\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paths and warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path = \"../data/eda_generated_data/\"\n",
    "output_path = \"../data/graph_data/\"\n",
    "\n",
    "def load_pickle(path, file_name):\n",
    "    with open(path + file_name, 'rb') as f: return pickle.load(f)\n",
    "    \n",
    "def save_pickle_file(file_name, file):\n",
    "    with open(output_path + file_name, 'wb') as f: pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_pickle(path, \"df_train.pkl\")\n",
    "df_val = load_pickle(path, \"df_val.pkl\")\n",
    "df_test = load_pickle(path, \"df_test.pkl\")\n",
    "scaler = load_pickle(path, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Previous application filtering and tranformation ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_df = pd.read_csv(\"../data/previous_application.csv\")\n",
    "df = pd.read_csv(\"../data/cleaning_generated_data/application_data_fraud.csv\")\n",
    "\n",
    "df_new_previous = previous_df[previous_df.SK_ID_CURR.isin(df.SK_ID_CURR)]\n",
    "\n",
    "\n",
    "null_values = df_new_previous.isna().sum()/len(df_new_previous)\n",
    "null_values = null_values[null_values > 0.3].index\n",
    "\n",
    "\n",
    "df_new_previous = df_new_previous.drop(null_values, axis=1)\n",
    "df_new_previous.dropna(inplace=True)\n",
    "df_new_previous.sort_values(by=['SK_ID_CURR'], inplace=True)\n",
    "\n",
    "numeric_previous = df_new_previous.select_dtypes(include=['float64', 'int64'])\n",
    "numeric_previous_no = numeric_previous.drop(['SK_ID_PREV','AMT_CREDIT','AMT_GOODS_PRICE'], axis=1)\n",
    "\n",
    "numeric_previous_no_pivoted = numeric_previous_no.pivot_table(index='SK_ID_CURR', aggfunc=['median', 'last', 'max', 'min'])\n",
    "numeric_previous_no_pivoted.columns = ['_'.join(col).strip() for col in numeric_previous_no_pivoted.columns.values]\n",
    "numeric_previous_no_pivoted.reset_index(inplace = True)\n",
    "\n",
    "df_product_combination = (pd.get_dummies(df_new_previous[['SK_ID_CURR', 'PRODUCT_COMBINATION', 'NAME_CONTRACT_STATUS']], columns = ['PRODUCT_COMBINATION', 'NAME_CONTRACT_STATUS'])\n",
    "                                        .groupby('SK_ID_CURR').sum()).reset_index()\n",
    "\n",
    "df_previous_graph_all = pd.merge(numeric_previous_no_pivoted, df_product_combination, on='SK_ID_CURR')\n",
    "save_pickle_file(\"df_previous_graph.pkl\", df_previous_graph_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2'>2. Split data</a>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the graph will become very large and complex, only 10.000 transactions will be used for the graph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_graph, _ = train_test_split(df_train, train_size = 16000, random_state = 40, stratify = df_train['TARGET'])\n",
    "df_val_graph, _ = train_test_split(df_val, train_size = 5000, random_state = 40, stratify = df_val['TARGET'])\n",
    "\n",
    "df_graph = pd.concat([df_train_graph, df_val_graph], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_file(\"df_train_graph.pkl\", df_train_graph)\n",
    "save_pickle_file(\"df_val_graph.pkl\", df_val_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Scaling previous ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_graph_train = df_previous_graph_all[df_previous_graph_all.SK_ID_CURR.isin(df_train.SK_ID_CURR)]\n",
    "scaler_previous = StandardScaler()\n",
    "scaler_previous.fit(df_previous_graph_train.drop(['SK_ID_CURR'], axis=1))\n",
    "save_pickle_file(\"scaler_previous.pkl\", scaler_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Previous with train data ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_graph_train = df_previous_graph_train[df_previous_graph_train.SK_ID_CURR.isin(df_train_graph.SK_ID_CURR)]\n",
    "df_previous_graph_val = df_previous_graph_all[df_previous_graph_all.SK_ID_CURR.isin(df_val_graph.SK_ID_CURR)]\n",
    "\n",
    "df_previous_graph = pd.concat([df_previous_graph_train, df_previous_graph_val], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Generation a dictionary of the ID and the position ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_index_dict = df_graph.SK_ID_CURR.to_dict()\n",
    "id_index_dict = {v: k for k, v in id_index_dict.items()}\n",
    "\n",
    "id_intersection = set(df_previous_graph.SK_ID_CURR).intersection(set(id_index_dict.keys()))\n",
    "dict_you_want = {key: id_index_dict[key] for key in id_intersection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_file(\"dict_you_want.pkl\", dict_you_want)\n",
    "save_pickle_file(\"id_index_dict.pkl\", id_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3'>3. Graph creation</a>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account that making relations of each feature individually would supose a huge amount of relations, the features are merged.   \n",
    "With the following function can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(df, combination_list):\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in combination_list:\n",
    "\n",
    "        df_with_selected_columns = df[list(i)]\n",
    "        name = '_'.join(i)\n",
    "        df_with_selected_columns[name] = df_with_selected_columns.apply(lambda x: ' '.join(x), axis=1)\n",
    "        new_df = pd.concat([new_df, df_with_selected_columns[name]], axis=1)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the manual features that have been agrupated. Moreover, it has also been done a rename dictionary to make the names more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_selection = [['ORGANIZATION_TYPE'], ['ORGANIZATION_TYPE', 'WEEKDAY_APPR_PROCESS_START'], ['NAME_INCOME_TYPE', 'FLAG_OWN_CAR', 'ORGANIZATION_TYPE'], \n",
    "                    ['FLAG_OWN_REALTY', 'ORGANIZATION_TYPE'], ['NAME_TYPE_SUITE', 'ORGANIZATION_TYPE'], ['NAME_CONTRACT_TYPE', 'ORGANIZATION_TYPE'], \n",
    "                    ['NAME_HOUSING_TYPE', 'NAME_EDUCATION_TYPE', 'ORGANIZATION_TYPE'], ['NAME_HOUSING_TYPE', 'NAME_FAMILY_STATUS', 'ORGANIZATION_TYPE']]\n",
    "                    \n",
    "rename_dict = {'ORGANIZATION_TYPE': 'organization', \n",
    "               'ORGANIZATION_TYPE_WEEKDAY_APPR_PROCESS_START': 'organization_weekday',\n",
    "               'NAME_INCOME_TYPE_FLAG_OWN_CAR_ORGANIZATION_TYPE': 'income_car_organization',\n",
    "               'FLAG_OWN_REALTY_ORGANIZATION_TYPE': 'realty_organization',\n",
    "               'NAME_TYPE_SUITE_ORGANIZATION_TYPE': 'suite_organization',\n",
    "               'NAME_CONTRACT_TYPE_ORGANIZATION_TYPE': 'contract_organization',\n",
    "               'NAME_HOUSING_TYPE_NAME_EDUCATION_TYPE_ORGANIZATION_TYPE': 'housing_education_organization',\n",
    "               'NAME_HOUSING_TYPE_NAME_FAMILY_STATUS_ORGANIZATION_TYPE': 'housing_family_organization'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is created and shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>organization</th>\n",
       "      <th>organization_weekday</th>\n",
       "      <th>income_car_organization</th>\n",
       "      <th>realty_organization</th>\n",
       "      <th>suite_organization</th>\n",
       "      <th>contract_organization</th>\n",
       "      <th>housing_education_organization</th>\n",
       "      <th>housing_family_organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XNA</td>\n",
       "      <td>XNA SATURDAY</td>\n",
       "      <td>Pensioner N XNA</td>\n",
       "      <td>Y XNA</td>\n",
       "      <td>Spouse, partner XNA</td>\n",
       "      <td>Cash loans XNA</td>\n",
       "      <td>House / apartment Secondary / secondary specia...</td>\n",
       "      <td>House / apartment Married XNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>Business Entity Type 3 THURSDAY</td>\n",
       "      <td>Working N Business Entity Type 3</td>\n",
       "      <td>Y Business Entity Type 3</td>\n",
       "      <td>Family Business Entity Type 3</td>\n",
       "      <td>Cash loans Business Entity Type 3</td>\n",
       "      <td>House / apartment Secondary / secondary specia...</td>\n",
       "      <td>House / apartment Married Business Entity Type 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>Business Entity Type 3 THURSDAY</td>\n",
       "      <td>Working Y Business Entity Type 3</td>\n",
       "      <td>Y Business Entity Type 3</td>\n",
       "      <td>Family Business Entity Type 3</td>\n",
       "      <td>Cash loans Business Entity Type 3</td>\n",
       "      <td>House / apartment Secondary / secondary specia...</td>\n",
       "      <td>House / apartment Single / not married Busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>Other SATURDAY</td>\n",
       "      <td>Working N Other</td>\n",
       "      <td>Y Other</td>\n",
       "      <td>Unaccompanied Other</td>\n",
       "      <td>Revolving loans Other</td>\n",
       "      <td>House / apartment Secondary / secondary specia...</td>\n",
       "      <td>House / apartment Married Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>Business Entity Type 3 WEDNESDAY</td>\n",
       "      <td>Commercial associate N Business Entity Type 3</td>\n",
       "      <td>Y Business Entity Type 3</td>\n",
       "      <td>Unaccompanied Business Entity Type 3</td>\n",
       "      <td>Cash loans Business Entity Type 3</td>\n",
       "      <td>House / apartment Secondary / secondary specia...</td>\n",
       "      <td>House / apartment Married Business Entity Type 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             organization              organization_weekday  \\\n",
       "0                     XNA                      XNA SATURDAY   \n",
       "1  Business Entity Type 3   Business Entity Type 3 THURSDAY   \n",
       "2  Business Entity Type 3   Business Entity Type 3 THURSDAY   \n",
       "3                   Other                    Other SATURDAY   \n",
       "4  Business Entity Type 3  Business Entity Type 3 WEDNESDAY   \n",
       "\n",
       "                         income_car_organization       realty_organization  \\\n",
       "0                                Pensioner N XNA                     Y XNA   \n",
       "1               Working N Business Entity Type 3  Y Business Entity Type 3   \n",
       "2               Working Y Business Entity Type 3  Y Business Entity Type 3   \n",
       "3                                Working N Other                   Y Other   \n",
       "4  Commercial associate N Business Entity Type 3  Y Business Entity Type 3   \n",
       "\n",
       "                     suite_organization              contract_organization  \\\n",
       "0                   Spouse, partner XNA                     Cash loans XNA   \n",
       "1         Family Business Entity Type 3  Cash loans Business Entity Type 3   \n",
       "2         Family Business Entity Type 3  Cash loans Business Entity Type 3   \n",
       "3                   Unaccompanied Other              Revolving loans Other   \n",
       "4  Unaccompanied Business Entity Type 3  Cash loans Business Entity Type 3   \n",
       "\n",
       "                      housing_education_organization  \\\n",
       "0  House / apartment Secondary / secondary specia...   \n",
       "1  House / apartment Secondary / secondary specia...   \n",
       "2  House / apartment Secondary / secondary specia...   \n",
       "3  House / apartment Secondary / secondary specia...   \n",
       "4  House / apartment Secondary / secondary specia...   \n",
       "\n",
       "                         housing_family_organization  \n",
       "0                      House / apartment Married XNA  \n",
       "1   House / apartment Married Business Entity Type 3  \n",
       "2  House / apartment Single / not married Busines...  \n",
       "3                    House / apartment Married Other  \n",
       "4   House / apartment Married Business Entity Type 3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = join(df_graph, manual_selection)\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "new_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "new_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following functions every relation is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations(new_df, feature, row):\n",
    "    filtrado = new_df[feature]\n",
    "    list_of_index_func = list(filtrado[filtrado == row[feature]].index)\n",
    "    \n",
    "    return list_of_index_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relations are:\n",
    "* **Self-loop:** The node is connected to itself.\n",
    "* **Bidirectional:** The node is connected to another node and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_creation(_user_list_user, new_df, test = None):\n",
    "    \n",
    "    if test == None: new_df_copy = new_df.copy()\n",
    "    else: new_df_copy = new_df.iloc[test:].copy()\n",
    "        \n",
    "    for k, row in enumerate(new_df_copy.iterrows()):\n",
    "        index, value = row\n",
    "\n",
    "        for x, cols in enumerate(new_df.columns): \n",
    "            list_of_index = get_relations(new_df, cols, value)\n",
    "\n",
    "            lenght_index = len(list_of_index)\n",
    "            _user_list_user[cols][0] += list_of_index\n",
    "            _user_list_user[cols][1] += list(np.full(lenght_index, index))\n",
    "\n",
    "    edges = [torch.tensor([np.array(v[0]), np.array(v[1])], dtype = torch.long) for k, v in _user_list_user.items()]\n",
    "\n",
    "    return edges\n",
    "\n",
    "_user_list_user = {i:[[],[]] for i in new_df.columns}\n",
    "edges = edge_creation(_user_list_user, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_user_has_previous = [[],[]]\n",
    "for k, v in dict_you_want.items():\n",
    "    df_graph_prev_filtered = df_previous_graph[df_previous_graph.SK_ID_CURR == k]\n",
    "    _user_has_previous[0].append(v)\n",
    "    _user_has_previous[1].append(df_graph_prev_filtered.index[0])\n",
    "\n",
    "edges_prev = torch.tensor([np.array(_user_has_previous[0]), np.array(_user_has_previous[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š X values of previous application ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prev = scaler_previous.transform(df_previous_graph.drop(['SK_ID_CURR'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric features are selected and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_numeric = df_graph.select_dtypes(include=['float64', 'int64'])\n",
    "df_graph_numeric_exclude = df_graph_numeric.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
    "df_graph_numeric_exclude_scaled = scaler.transform(df_graph_numeric_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š With all the information the graph is created. ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1musers\u001b[0m={\n",
       "    x=[21000, 48],\n",
       "    y=[21000]\n",
       "  },\n",
       "  \u001b[1mprevious\u001b[0m={ x=[20036, 48] },\n",
       "  \u001b[1m(users, organization, users)\u001b[0m={ edge_index=[2, 46304716] },\n",
       "  \u001b[1m(users, organization_weekday, users)\u001b[0m={ edge_index=[2, 7233784] },\n",
       "  \u001b[1m(users, income_car_organization, users)\u001b[0m={ edge_index=[2, 19286166] },\n",
       "  \u001b[1m(users, realty_organization, users)\u001b[0m={ edge_index=[2, 27157012] },\n",
       "  \u001b[1m(users, suite_organization, users)\u001b[0m={ edge_index=[2, 31595802] },\n",
       "  \u001b[1m(users, contract_organization, users)\u001b[0m={ edge_index=[2, 38976610] },\n",
       "  \u001b[1m(users, housing_education_organization, users)\u001b[0m={ edge_index=[2, 22939114] },\n",
       "  \u001b[1m(users, housing_family_organization, users)\u001b[0m={ edge_index=[2, 16688072] },\n",
       "  \u001b[1m(users, has_previous, previous)\u001b[0m={ edge_index=[2, 20036] },\n",
       "  \u001b[1m(previous, rev_has_previous, users)\u001b[0m={ edge_index=[2, 20036] }\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = HeteroData()\n",
    "\n",
    "datas['users'].x = torch.from_numpy(df_graph_numeric_exclude_scaled).float()\n",
    "datas['users'].y = torch.from_numpy(df_graph_numeric.TARGET.values).long()\n",
    "\n",
    "for k, v in zip(new_df.columns, edges): datas['users', k, 'users'].edge_index = v\n",
    "\n",
    "datas['previous'].x = torch.from_numpy(x_prev).float()\n",
    "datas['users', 'has_previous', 'previous'].edge_index = edges_prev\n",
    "\n",
    "datas = T.ToUndirected()(datas)\n",
    "datas = T.AddSelfLoops()(datas)\n",
    "datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally train and validation masks are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, val_mask = np.array([True] * 16000 + [False] * 5000), np.array([False] * 16000 + [True] * 5000)\n",
    "\n",
    "datas['users'].train_mask = torch.from_numpy(train_mask).bool()\n",
    "datas['users'].valid_mask = torch.from_numpy(train_mask).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4'>4. Graph save</a>\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(datas, output_path + 'training_graph_prev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle_file(file_name, file):\n",
    "    with open(output_path + file_name, 'wb') as f: pickle.dump(file, f)\n",
    "\n",
    "save_pickle_file('graph_df.pkl', new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='5'>5. New customer inclusion</a>\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = torch.load('../data/graph_data/training_graph_prev.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train preprocessing is applied to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_test(df_test):\n",
    "    df_test_cleaned = df_test[df_train.columns]\n",
    "    return df_test_cleaned\n",
    "\n",
    "df_test_cleaned = cleaning_test(df_test)\n",
    "df_test_cleaned = df_test_cleaned.iloc[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical columns are selected and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned_numeric = df_test_cleaned.select_dtypes(include=['float64', 'int64'])\n",
    "df_test_cleaned_numeric_exclude = df_test_cleaned_numeric.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
    "df_test_cleaned_numeric_exclude_scaled = scaler.transform(df_test_cleaned_numeric_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled values are added to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_values = torch.from_numpy(df_test_cleaned_numeric_exclude_scaled).float()\n",
    "datas['users'].x = torch.cat((datas['users'].x, new_x_values))\n",
    "\n",
    "last_val = datas['users', 'organization', 'users']['edge_index'][1,-1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged dataframe is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned.index = range(last_val+1, last_val+1 + len(df_test_cleaned))\n",
    "df_test_cleaned_colums_agregated = join(df_test_cleaned, manual_selection)\n",
    "df_test_cleaned_colums_agregated.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges are obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = pd.concat([new_df, df_test_cleaned_colums_agregated], axis=0)\n",
    "_user_list_user, length_test = {i:[[],[]] for i in new_df2.columns}, len(df_test_cleaned_colums_agregated)\n",
    "\n",
    "new_edges = edge_creation(_user_list_user, new_df2, -length_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Edges for previous application in test ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_graph_test = df_previous_graph_all[df_previous_graph_all.SK_ID_CURR.isin(df_test_cleaned.SK_ID_CURR)]\n",
    "df_previous_graph_test.index = range(len(dict_you_want), len(dict_you_want) + len(df_previous_graph_test))\n",
    "\n",
    "max_dict_value = max(id_index_dict.values())\n",
    "id_index_dict_test = {k:v for k, v in zip(df_previous_graph_test.SK_ID_CURR, range(max_dict_value + 1, max_dict_value + len(df_previous_graph_test) + 1))}\n",
    "id_index_dict_new = {**id_index_dict, **id_index_dict_test}\n",
    "\n",
    "_user_has_previous_test = [[],[]]\n",
    "for k, v in id_index_dict_test.items():\n",
    "    df_graph_prev_filtered = df_previous_graph_test[df_previous_graph_test.SK_ID_CURR == k]\n",
    "    _user_has_previous_test[0].append(v)\n",
    "    _user_has_previous_test[1].append(df_graph_prev_filtered.index[0])\n",
    "\n",
    "edges_prev_test = torch.tensor([np.array(_user_has_previous_test[0]), np.array(_user_has_previous_test[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Previous application x values in test ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_prev_values = scaler_previous.transform(df_previous_graph_test.drop(['SK_ID_CURR'], axis=1))\n",
    "datas['previous'].x = torch.cat((datas['previous'].x, torch.from_numpy(new_x_prev_values).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š A temporal merged graph is done so as to create biderectional relations and then contatenate with the big one. ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_merge = HeteroData()\n",
    "\n",
    "for k, v in zip(new_df2.columns, new_edges): datas_merge['users', k, 'users'].edge_index = v\n",
    "\n",
    "datas_merge['users', 'has_previous', 'previous'].edge_index = edges_prev_test\n",
    "\n",
    "datas_merge = T.ToUndirected()(datas_merge)\n",
    "datas_merge = T.AddSelfLoops()(datas_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“š Edges are merged ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in enumerate(new_df2.columns):\n",
    "    previous_relation, new_relations = datas['users', v, 'users'].edge_index, datas_merge['users', v, 'users'].edge_index\n",
    "    new_relation = torch.cat((previous_relation, new_relations), dim=1)\n",
    "    datas['users', v, 'users'].edge_index = new_relation\n",
    "\n",
    "previous_relation, new_relations = datas['users', 'has_previous', 'previous'].edge_index, datas_merge['users', 'has_previous', 'previous'].edge_index\n",
    "new_relation = torch.cat((previous_relation, new_relations), dim=1)\n",
    "datas['users', 'has_previous', 'previous'].edge_index = new_relation\n",
    "\n",
    "previous_relation_rev, new_relations_rev = datas['previous', 'rev_has_previous', 'users'].edge_index, datas_merge['previous', 'rev_has_previous', 'users'].edge_index\n",
    "new_relation_rev = torch.cat((previous_relation_rev, new_relations_rev), dim=1)\n",
    "datas['previous', 'rev_has_previous', 'users'].edge_index = new_relation_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test mask is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_mask = np.array([False] * (last_val+1) + [True] * len(df_test_cleaned_colums_agregated))\n",
    "datas['users'].test_mask = torch.from_numpy(test_new_mask).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRYIALS** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../data/models/sage_heterogen_model_prev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(datas.x_dict, datas.edge_index_dict)\n",
    "\n",
    "mask = datas['users'].test_mask\n",
    "pred = out['users'][mask].max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6585472129231096,\n",
       " 0.24780058651026396,\n",
       " 0.1549511002444988,\n",
       " 0.6182926829268293)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_test.iloc[:10000].TARGET, pred), f1_score(df_test.iloc[:10000].TARGET, pred), precision_score(df_test.iloc[:10000].TARGET, pred), recall_score(df_test.iloc[:10000].TARGET, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6416281417716139,\n",
       " 0.24770642201834858,\n",
       " 0.1619190404797601,\n",
       " 0.526829268292683)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_test.iloc[:10000].TARGET, pred), f1_score(df_test.iloc[:10000].TARGET, pred), precision_score(df_test.iloc[:10000].TARGET, pred), recall_score(df_test.iloc[:10000].TARGET, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\tfg_mlflow\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "import pickle\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "                                \n",
    "def save_pickle_file(file_name, file):\n",
    "    with open(output_path + file_name, 'wb') as f: pickle.dump(file, f)\n",
    "\n",
    "def load_pickle(path, file_name):\n",
    "    with open(path + file_name, 'rb') as f: return pickle.load(f)\n",
    "\n",
    "path = \"../data/graph_data/\"\n",
    "eda_path = \"../data/eda_generated_data/\"\n",
    "output_path = \"../data/models/\"\n",
    "\n",
    "data = torch.load(path + 'training_graph_prev.pt')\n",
    "model = torch.load('../data/models/sage_heterogen_model_prev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain.explainer import Explainer\n",
    "from torch_geometric.explain.algorithm import GNNExplainer, base\n",
    "from torch_geometric.explain import Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  1\n",
      "Epochs:  2\n",
      "Epochs:  3\n",
      "Epochs:  4\n",
      "Epochs:  5\n",
      "Epochs:  1\n",
      "Epochs:  2\n",
      "Epochs:  3\n",
      "Epochs:  4\n",
      "Epochs:  5\n",
      "Epochs:  1\n",
      "Epochs:  2\n",
      "Epochs:  3\n",
      "Epochs:  4\n",
      "Epochs:  5\n"
     ]
    }
   ],
   "source": [
    "node_mask_type = [\"attributes\", \"object\", \"common_attributes\"]\n",
    "explanation_list = []\n",
    "for i in node_mask_type:\n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "        algorithm=GNNExplainer(epochs=5),\n",
    "        explainer_config=dict(\n",
    "            explanation_type = \"model\",\n",
    "            node_mask_type = i,\n",
    "        ),\n",
    "        model_config=dict(\n",
    "            mode='classification',\n",
    "            task_level='node',\n",
    "            return_type='probs',\n",
    "        ),\n",
    "        \n",
    "    )\n",
    "\n",
    "    node_idx = 10\n",
    "    explanation = explainer(data.x_dict, data.edge_index_dict, index=node_idx)\n",
    "    explanation_list.append(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The node with the highest value in a feature is: 18842 with a value of 0.6036412715911865\n",
      "Most important node: 3994, with an importance of 0.5811887383460999\n",
      "Most important feature: 22 with an importance of 0.5546156167984009\n"
     ]
    }
   ],
   "source": [
    "print(f\"The node with the highest value in a feature is: {np.argmax((np.array(explanation_list[0].node_feat_mask)).max(axis=1))} with a value of {np.amax(np.array(explanation_list[0].node_feat_mask))}\")\n",
    "print(f\"Most important node: {int(np.argmax(explanation_list[1].node_mask))}, with an importance of {float(max(explanation_list[1].node_mask))}\")\n",
    "print(f\"Most important feature: {int(np.argmax(explanation_list[2].node_feat_mask[1]))} with an importance of {float(max(explanation_list[2].node_feat_mask[1]))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('tfg_mlflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1edfdd4f2ba434fa34349e35660af1516eb69f8b7fb896a2cf404b168d66bee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
